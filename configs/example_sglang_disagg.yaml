# Example SGLang disaggregated configuration with config file support
# This demonstrates the new YAML-based approach

name: "sglang-disagg-example"

# SLURM settings
slurm:
  account: "your-account"
  partition: "gpu"
  time_limit: "02:00:00"

# Resource allocation
resources:
  prefill_nodes: 1
  decode_nodes: 12
  prefill_workers: 1
  decode_workers: 1
  gpus_per_node: 4

# Model configuration
model:
  path: "/models/deepseek-r1"
  container: "/containers/sglang.sqsh"

# Backend configuration (NEW: config file approach)
backend:
  type: "sglang"  # Use SGLang config file mode

  # Environment variables (applied to all workers)
  # These replace the env vars scattered in GPU scripts
  environment:
    # SGLang-specific
    DYN_SKIP_SGLANG_LOG_FORMATTING: "1"
    SGLANG_NVFP4_CKPT_FP8_GEMM_IN_ATTN: "1"
    SGLANG_PER_TOKEN_GROUP_QUANT_8BIT_V2: "1"
    SGLANG_DISAGGREGATION_HEARTBEAT_MAX_FAILURE: "100000"
    SGLANG_DISAGGREGATION_BOOTSTRAP_TIMEOUT: "100000"
    SGLANG_DISAGGREGATION_WAITING_TIMEOUT: "100000"
    SGLANG_HACK_SEQ_BOOTSTRAP_ROOM: "1"
    SGLANG_MOONCAKE_CUSTOM_MEM_POOL: "True"
    SGLANG_USE_MESSAGE_QUEUE_BROADCASTER: "0"
    SGLANG_DISABLE_TP_MEMORY_INBALANCE_CHECK: "1"
    PYTHONUNBUFFERED: "1"

    # NCCL settings
    MC_TE_METRIC: "true"
    MC_FORCE_MNNVL: "1"
    NCCL_MNNVL_ENABLE: "1"
    NCCL_CUMEM_ENABLE: "1"

    # PyTorch
    TORCH_DISTRIBUTED_DEFAULT_TIMEOUT: "1800"

  # SGLang config file (replaces 50+ CLI flags!)
  sglang_config:
    # Shared between prefill and decode
    shared:
      served_model_name: "deepseek-ai/DeepSeek-R1"
      model_path: "/model/"
      trust_remote_code: true
      host: "0.0.0.0"
      disable_radix_cache: true
      disable_shared_experts_fusion: true
      watchdog_timeout: 1000000
      disable_chunked_prefix_cache: true
      attention_backend: "trtllm_mla"
      kv_cache_dtype: "fp8_e4m3"
      enable_single_batch_overlap: true
      eplb_algorithm: "deepseek"
      disable_cuda_graph: true
      quantization: "modelopt_fp4"
      moe_runner_backend: "flashinfer_cutlass"
      disaggregation_bootstrap_port: 30001
      enable_dp_attention: true
      stream_interval: 50
      log_level: "debug"

    # Prefill-specific flags
    prefill:
      disaggregation_mode: "prefill"
      decode_log_interval: 1000
      max_running_requests: 30000
      context_length: 2176
      chunked_prefill_size: 65536
      mem_fraction_static: 0.84
      max_total_tokens: 131072
      max_prefill_tokens: 32768
      load_balance_method: "round_robin"

    # Decode-specific flags
    decode:
      disaggregation_mode: "decode"
      decode_log_interval: 1000
      max_running_requests: 30000
      context_length: 2176
      chunked_prefill_size: 131072
      mem_fraction_static: 0.85
      max_total_tokens: 262144
      max_prefill_tokens: 65536
      load_balance_method: "round_robin"

# Benchmark configuration
benchmark:
  type: "sa-bench"
  isl: 1024
  osl: 1024
  concurrencies: [1024, 2048]
  req_rate: "inf"
