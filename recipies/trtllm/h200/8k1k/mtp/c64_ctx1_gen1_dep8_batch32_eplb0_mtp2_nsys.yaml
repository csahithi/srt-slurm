name: "c64_ctx1_gen1_dep8_batch32_eplb0_mtp2_nsys"

model:
  path: "/models/DeepSeek-R1-0528"
  container: "nvcr.io#nvidia/ai-dynamo/tensorrtllm-runtime:0.8.0"
  precision: "fp8"

# Mount nsys from host (not in container by default)
extra_mount:
  - "/opt/nvidia/nsight-systems/2025.1.3:/opt/nvidia/nsight-systems"
  - "/usr/local/bin/nsys:/usr/local/bin/nsys"

sbatch_directives:
  cpus-per-gpu: "16"

resources:
  gpu_type: "h200"
  prefill_nodes: 1
  prefill_workers: 1
  decode_workers: 1
  decode_nodes: 1
  gpus_per_node: 8

# Enable nsys profiling (1:1 ratio - single worker per role)
profiling:
  type: "nsys"
  isl: 8192
  osl: 1024
  concurrency: 64   # Matches this config's target concurrency
  num_prompts: 256  # Enough to stress c=64 (4x concurrency)
  prefill:
    start_step: 10   # Skip CUDA graph capture warmup
    stop_step: 140   # Capture ~130 prefill ops (covers 256 prompts with batch=2)
  decode:
    start_step: 100  # Skip batch ramp-up, capture steady state
    stop_step: 500   # 400 decode iterations with MTP2

backend:
  type: trtllm

  prefill_environment:
    UCX_TLS: "rc,dc,ud,cuda_copy,cuda_ipc,gdr_copy,tcp"
    TRTLLM_ENABLE_PDL: "1"
    TRTLLM_SERVER_DISABLE_GC: "1"
    TRTLLM_WORKER_DISABLE_GC: "1"
    NCCL_GRAPH_MIXING_SUPPORT: "0"

  decode_environment:
    UCX_TLS: "rc,dc,ud,cuda_copy,cuda_ipc,gdr_copy,tcp"
    TRTLLM_ENABLE_PDL: "1"
    TRTLLM_SERVER_DISABLE_GC: "1"
    TRTLLM_WORKER_DISABLE_GC: "1"
    NCCL_GRAPH_MIXING_SUPPORT: "0"

  trtllm_config:
    prefill:
      # Prefill Worker Config for Dynamo DSR1 (MTP mode)
      # ISL/OSL: 8k/1k, TP=8 on H200
      backend: pytorch
      trust_remote_code: true
      tensor_parallel_size: 8
      moe_expert_parallel_size: 8
      pipeline_parallel_size: 1
      enable_attention_dp: false
      enable_chunked_prefill: false
      max_batch_size: 2
      max_num_tokens: 16640
      max_seq_len: 8232
      kv_cache_config:
        enable_block_reuse: false
        free_gpu_memory_fraction: 0.85
        dtype: fp8
      cache_transceiver_config:
        backend: UCX
        max_tokens_in_buffer: 32768
      moe_config:
        backend: CUTLASS
      cuda_graph_config: null
      disable_overlap_scheduler: true
      print_iter_log: true
      # Performance tuning
      stream_interval: 100
      num_postprocess_workers: 4
      speculative_config:
        decoding_type: MTP
        num_nextn_predict_layers: 2
    decode:
      # Decode Worker Config for Dynamo DSR1 (MTP c=64)
      # ISL/OSL: 8k/1k, TP=8 on H200
      backend: pytorch
      trust_remote_code: true
      tensor_parallel_size: 8
      moe_expert_parallel_size: 8
      pipeline_parallel_size: 1
      enable_attention_dp: true
      enable_chunked_prefill: false
      max_batch_size: 32
      max_num_tokens: 128
      max_seq_len: 9256
      kv_cache_config:
        enable_block_reuse: false
        free_gpu_memory_fraction: 0.85
        dtype: fp8
      cache_transceiver_config:
        backend: UCX
        max_tokens_in_buffer: 16384
      moe_config:
        backend: CUTLASS
        use_low_precision_moe_combine: true
      cuda_graph_config:
        enable_padding: true
        batch_sizes: [1, 2, 4, 8, 16, 32]
      disable_overlap_scheduler: false
      print_iter_log: true
      # Performance tuning
      stream_interval: 100
      num_postprocess_workers: 4
      speculative_config:
        decoding_type: MTP
        num_nextn_predict_layers: 2

# Must be "manual" when profiling is enabled
benchmark:
  type: "manual"

frontend:
  type: "dynamo"
  enable_multiple_frontends: false

dynamo:
  install: false
