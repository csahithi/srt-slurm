name: "h100-wideep-blog-post"

model:
  path: "dsfp8"
  container: "0.5.5.post2"
  precision: "fp8"

# same config as blog post
# but rn keeping it at 2 nodes per worker for pipecleaning
resources:
  gpu_type: "h100"
  prefill_nodes: 2
  decode_nodes: 2
  prefill_workers: 1
  decode_workers: 1
  gpus_per_node: 8

backend:

  prefill_environment:
    MC_TE_METRIC: "true"
    SGLANG_TBO_DEBUG: "1"
    TORCH_DISTRIBUTED_DEFAULT_TIMEOUT: "1800"
    SGLANG_DG_CACHE_DIR: "/configs/dg-10212025"
    DYN_SKIP_SGLANG_LOG_FORMATTING: "1"
    SGLANG_DISAGGREGATION_HEARTBEAT_MAX_FAILURE: "100000"
    SGLANG_DISAGGREGATION_BOOTSTRAP_TIMEOUT: "100000"
    SGLANG_DISAGGREGATION_WAITING_TIMEOUT: "100000"
    SGLANG_USE_MESSAGE_QUEUE_BROADCASTER: "0"
    SGLANG_DISABLE_TP_MEMORY_INBALANCE_CHECK: "1"
    PYTHONUNBUFFERED: "1"

  decode_environment:
    MC_TE_METRIC: "true"
    SGLANG_TBO_DEBUG: "1"
    TORCH_DISTRIBUTED_DEFAULT_TIMEOUT: "1800"
    SGLANG_DG_CACHE_DIR: "/configs/dg-10212025"
    DYN_SKIP_SGLANG_LOG_FORMATTING: "1"
    SGLANG_DISAGGREGATION_HEARTBEAT_MAX_FAILURE: "100000"
    SGLANG_DISAGGREGATION_BOOTSTRAP_TIMEOUT: "100000"
    SGLANG_DISAGGREGATION_WAITING_TIMEOUT: "100000"
    SGLANG_USE_MESSAGE_QUEUE_BROADCASTER: "0"
    SGLANG_DEEPEP_NUM_MAX_DISPATCH_TOKENS_PER_RANK: "768"
    SGLANG_DISABLE_TP_MEMORY_INBALANCE_CHECK: "1"
    PYTHONUNBUFFERED: "1"

  sglang_config:
    prefill:
      # Model configuration
      served-model-name: "deepseek-ai/DeepSeek-R1"
      model-path: "/model/"
      skip-tokenizer-init: true
      trust-remote-code: true
      disaggregation-mode: "prefill"
      load-balance-method: "round_robin"

      # Parallelism
      tp-size: 16
      dp-size: 16
      ep-size: 16
      enable-dp-attention: true

      # network
      host: "0.0.0.0"
      disaggregation-bootstrap-port: 30001
      watchdog-timeout: 1000000

      # other
      disable-radix-cache: true
      enable-two-batch-overlap: true
      mem-fraction-static: 0.85

      # chunked-prefill-size: 524288
      # max-running-requests 8192
      # max-total-tokens 131072
      # context-length 8192

      # moe
      moe-a2a-backend: "deepep"
      deepep-mode: "normal"
      ep-dispatch-algorithm: "static"
      moe-dense-tp-size: 1
      enable-dp-lm-head: true
      ep-num-redundant-experts: 32
      eplb-algorithm: "deepseek"

    decode:
      # Model configuration
      served-model-name: "deepseek-ai/DeepSeek-R1"
      model-path: "/model/"
      skip-tokenizer-init: true
      trust-remote-code: true
      disaggregation-mode: "decode"
      prefill-round-robin-balance: true
      
      # Parallelism
      tp-size: 16
      dp-size: 16
      ep-size: 16
      enable-dp-attention: true

      # network
      host: "0.0.0.0"
      disaggregation-bootstrap-port: 30001
      watchdog-timeout: 1000000

      # other
      disable-radix-cache: true
      enable-two-batch-overlap: true
      mem-fraction-static: 0.835
      cuda-graph-bs: 8

      # moe
      moe-a2a-backend: "deepep"
      deepep-mode: "low_latency"
      ep-dispatch-algorithm: "static"
      moe-dense-tp-size: 1
      enable-dp-lm-head: true
      ep-num-redundant-experts: 32
      eplb-algorithm: "deepseek"

      # max-running-requests 18432
      # context-length 4500

