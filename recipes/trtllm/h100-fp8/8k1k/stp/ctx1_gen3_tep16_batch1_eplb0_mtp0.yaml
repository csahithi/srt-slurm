name: h100_8k1k_ctx1dep16_gen3tep16_batch1_eplb0_mtp0
model:
  path: DeepSeek-R1-0528
  container: "nvcr.io#nvidia/ai-dynamo/tensorrtllm-runtime:0.8.1.post3"
  precision: fp8
resources:
  gpu_type: h100
  prefill_workers: 1
  prefill_nodes: 2
  decode_workers: 3
  decode_nodes: 6
  gpus_per_node: 8
backend:
  type: trtllm
  prefill_environment:
    UCX_CUDA_IPC_ENABLE_MNNVL: n
    TRTLLM_ENABLE_PDL: '1'
    TRTLLM_SERVER_DISABLE_GC: '1'
    TRTLLM_WORKER_DISABLE_GC: '1'
    NCCL_GRAPH_MIXING_SUPPORT: '0'
    TLLM_LOG_LEVEL: INFO
    TRTLLM_DISABLE_KV_CACHE_TRANSFER_OVERLAP: '1'
    TRTLLM_FORCE_ALLTOALL_METHOD: DeepEP
  decode_environment:
    NCCL_NVLS_ENABLE: '0'
    UCX_CUDA_IPC_ENABLE_MNNVL: n
    TRTLLM_ENABLE_PDL: '1'
    TRTLLM_SERVER_DISABLE_GC: '1'
    TRTLLM_WORKER_DISABLE_GC: '1'
    NCCL_GRAPH_MIXING_SUPPORT: '0'
    TLLM_LOG_LEVEL: INFO
    TRTLLM_DISABLE_KV_CACHE_TRANSFER_OVERLAP: '1'
  trtllm_config:
    prefill:
      max_batch_size: 1
      max_num_tokens: 8224
      max_seq_len: 8232
      tensor_parallel_size: 16
      moe_expert_parallel_size: 16
      enable_attention_dp: true
      pipeline_parallel_size: 1
      print_iter_log: true
      cuda_graph_config: null
      disable_overlap_scheduler: true
      enable_chunked_prefill: false
      moe_config:
        backend: WIDEEP
        max_num_tokens: 16384
      kv_cache_config:
        enable_block_reuse: false
        free_gpu_memory_fraction: 0.3
        dtype: fp8
      cache_transceiver_config:
        backend: UCX
        max_tokens_in_buffer: 8256
    decode:
      tensor_parallel_size: 16
      moe_expert_parallel_size: 16
      enable_attention_dp: false
      enable_lm_head_tp_in_adp: false
      pipeline_parallel_size: 1
      max_batch_size: 1
      max_num_tokens: 256
      max_seq_len: 9256
      cuda_graph_config:
        enable_padding: true
        batch_sizes:
        - 1
        - 2
        - 4
      print_iter_log: true
      kv_cache_config:
        enable_block_reuse: false
        free_gpu_memory_fraction: 0.9
        dtype: fp8
      moe_config:
        backend: CUTLASS
        use_low_precision_moe_combine: true
      cache_transceiver_config:
        max_tokens_in_buffer: 8256
        backend: UCX
      stream_interval: 100
      num_postprocess_workers: 4
benchmark:
  type: sa-bench
  isl: 8192
  osl: 1024
  concurrencies: '6'
  req_rate: inf
frontend:
  type: dynamo
  enable_multiple_frontends: false
dynamo:
  install: false
