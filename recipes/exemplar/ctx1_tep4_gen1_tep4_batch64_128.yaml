name: ctx1_tep4_gen1_tep4_batch64_128  #p64 d128

model:
  path: "deepseek-v32"
  container: "nvcr.io#nvidia/ai-dynamo/tensorrtllm-runtime:0.8.0"
  precision: "fp8"

resources:
  gpu_type: "gb200"
  prefill_nodes: 1
  prefill_workers: 1
  gpus_per_prefill: 4

  decode_nodes: 1
  decode_workers: 1
  gpus_per_decode: 4

  gpus_per_node: 4

backend:
  type: trtllm

  prefill_environment:
    TLLM_LOG_LEVEL: "INFO"
    TRTLLM_SERVER_DISABLE_GC: "1"
    TRTLLM_WORKER_DISABLE_GC: "1"
    TRTLLM_ENABLE_PDL: "1"
    ENROOT_ALLOW_DEV: "yes"
    NCCL_GRAPH_MIXING_SUPPORT: "0"

  decode_environment:
    TLLM_LOG_LEVEL: "INFO"
    TRTLLM_SERVER_DISABLE_GC: "1"
    TRTLLM_WORKER_DISABLE_GC: "1"
    TRTLLM_ENABLE_PDL: "1"
    ENROOT_ALLOW_DEV: "yes"
    NCCL_GRAPH_MIXING_SUPPORT: "0"
    TRTLLM_FORCE_COMM_METHOD: "NVLINK_TWO_SIDED"
    ENABLE_CONFIGURABLE_MOE: "1"

  trtllm_config:
    prefill:
      cache_transceiver_config:
        backend: UCX
        max_tokens_in_buffer: 16384
      enable_chunked_prefill: true
      cuda_graph_config: 
        max_batch_size: 64
        enable_padding: true
      disable_overlap_scheduler: true
      enable_attention_dp: false
      kv_cache_config:
        dtype: fp8
        enable_block_reuse: false
        free_gpu_memory_fraction: 0.8
      max_batch_size: 64                   # max bs 4
      max_num_tokens: 40000
      max_seq_len: 42000                 # max seq len 128k 
      moe_config:
        backend: CUTLASS
      moe_expert_parallel_size: 1         # EP4
      expert_parallel_size: 4           # ^ EP4 but idk which flag is right  
      print_iter_log: true
      tensor_parallel_size: 4             # TP4

    decode:
      allreduce_strategy: MNNVL
      cache_transceiver_config:
        backend: UCX
        max_tokens_in_buffer: 16384
      max_num_tokens: 16384
      cuda_graph_config:
        max_batch_size: 128
        enable_padding: true
      enable_attention_dp: true
      kv_cache_config:
        dtype: fp8
        enable_block_reuse: false
        free_gpu_memory_fraction: 0.9
      max_batch_size: 128                   # max bs 128
      max_seq_len: 128000                 # max seq len 128k (should it be higher?)
      moe_config:
        backend: CUTLASS
        use_low_precision_moe_combine: true
      moe_expert_parallel_size: 4         # EP4
      expert_parallel_size: 4           # ^ EP4 but idk which flag is right  
      num_postprocess_workers: 4
      print_iter_log: true
      stream_interval: 100
      tensor_parallel_size: 4             # TP4



benchmark:
  type: "sa-bench"
  isl: 120000 
  osl: 8000
  concurrencies: ['4','16','32','64','128','256']
  req_rate: "inf"

frontend:
  type: "dynamo"
  enable_multiple_frontends: false


health_check:
  max_attempts: 360
  interval_seconds: 10

dynamo:
  install: false