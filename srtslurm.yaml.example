# Example cluster configuration (srtslurm.yaml)
# Copy this to srtslurm.yaml and customize for your cluster
# This file is gitignored to keep cluster-specific settings private
#
# Discovery: srtctl searches cwd and up to 2 parent directories.
# For deep directory structures, set SRTSLURM_CONFIG in your shell:
#   export SRTSLURM_CONFIG="/path/to/srt-slurm/srtslurm.yaml"

# SLURM defaults
default_account: "your-gpu-account"
default_partition: "gpu"
default_time_limit: "04:00:00"

# SLURM directive compatibility
# Set to false if your cluster doesn't support --gpus-per-node
use_gpus_per_node_directive: true  # Default: true
# Set to false if your cluster doesn't support --segment for segment-based scheduling
use_segment_sbatch_directive: true  # Default: true
# Set to true if your cluster requires --exclusive to be set.
use_exclusive_sbatch_directive: false # Default: false
# Container registry
# Two formats are supported:
#   1. Simple string (path only, for existing containers)
#   2. Dict with path and source (enables `srtctl container-pull`)
default_container: "/shared/containers/sglang-latest.sqsh"
containers:
  # Old format (still works) - for existing containers
  sglang-latest: "/shared/containers/sglang-v0.4.sqsh"
  sglang-fp4: "/shared/containers/sglang-fp4.sqsh"
  nginx: "/shared/containers/nginx.sqsh"

  # New format (with source) - enables `srtctl container-pull`
  sglang-dev:
    path: "/shared/containers/sglang-dev.sqsh"
    source: "docker://nvcr.io/nvidia/sglang:dev"

  # Example with versioned image
  sglang-0.4.1:
    path: "/shared/containers/sglang-v0.4.1.sqsh"
    source: "docker://nvcr.io/nvidia/sglang:0.4.1"

# Run `srtctl container-pull` to download containers via batch job.
# Use `srtctl container-pull --force` to re-download existing containers.
# Use `srtctl container-pull --local` to run directly on login node.

# Model path aliases (both 'models' and 'model_paths' work)
models:
  deepseek-r1: "/shared/models/deepseek/DeepSeek-R1"
  deepseek-r1-distill: "/shared/models/deepseek/DeepSeek-R1-Distill-Qwen-32B"
  llama-3-70b: "/shared/models/meta/llama-3-70b"
  llama-3-405b: "/shared/models/meta/llama-3-405b"

# AI-powered failure analysis (optional)
# When enabled, Claude Code CLI analyzes benchmark failures and writes ai_analysis.md
# Uses OpenRouter for authentication (works well in headless/automated environments)
# See: https://openrouter.ai/docs/guides/guides/claude-code-integration
# ai_analysis:
#   enabled: true
#   
#   # Authentication (required - can also use env vars OPENROUTER_API_KEY / GH_TOKEN)
#   openrouter_api_key: "sk-or-v1-..."
#   gh_token: "ghp_..."
#   
#   # GitHub repos to search for related PRs when analyzing failures
#   repos_to_search:
#     - sgl-project/sglang
#     - ai-dynamo/dynamo
#   pr_search_days: 14
#   
#   # Custom prompt template (optional - uses sensible default if omitted)
#   # Available variables: {log_dir}, {repos}, {pr_days}
#   # prompt: |
#   #   You are analyzing benchmark failure logs for an LLM serving system.
#   #   ...

# Usage:
# In your job config, you can now use:
#   model:
#     path: "deepseek-r1"         # Resolves to /shared/models/deepseek/DeepSeek-R1
#     container: "sglang-fp4"     # Resolves to /shared/containers/sglang-fp4.sqsh
#
# And you can omit SLURM fields:
#   slurm:
#     time_limit: "02:00:00"      # account and partition filled from defaults
